{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576ae60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0         hashed_userid masked_username         location  \\\n",
      "0      415371  19868647935216335990       *****ecot              NaN   \n",
      "1      415370  95273352056344375133       *****kh59  Terre Haute, IN   \n",
      "2      415369  42256911176251501556  *******eDuster         Chi-town   \n",
      "3      415368  98949018742144878760       *****ll42              NaN   \n",
      "4      415367  83242079331442835051  *******tresist              NaN   \n",
      "\n",
      "   following  followers  totaltweets usercreateddt              tweetid  \\\n",
      "0       1109        796       189199       9/16/09  1560823411047268352   \n",
      "1        854        298        16999      10/10/17  1560823347583361024   \n",
      "2        416       8852        11699       3/26/09  1560823151671488513   \n",
      "3        603        179        59766       3/14/14  1560822898780213249   \n",
      "4        257        758       272531       2/16/17  1560822761706094592   \n",
      "\n",
      "  tweetcreatedts  ...                                           hashtags  \\\n",
      "0        57:54.0  ...  [{'text': 'MyBodyMyChoice', 'indices': [200, 2...   \n",
      "1        57:39.0  ...  [{'text': 'mybodymychoice', 'indices': [55, 70...   \n",
      "2        56:52.0  ...  [{'text': 'OnThisDay', 'indices': [32, 42]}, {...   \n",
      "3        55:52.0  ...  [{'text': 'NY', 'indices': [24, 27]}, {'text':...   \n",
      "4        55:19.0  ...     [{'text': 'WomensRights', 'indices': [0, 13]}]   \n",
      "\n",
      "  language favorite_count is_retweet    original_tweet_id  \\\n",
      "0       en              0       True  1560766115776565248   \n",
      "1       en              0      False                    0   \n",
      "2       en              0       True  1560280488488718336   \n",
      "3       en              0       True  1560750229720563712   \n",
      "4       en              0       True  1560794896663203840   \n",
      "\n",
      "   in_reply_to_status_id  is_quote_status  quoted_status_id  extractedts  \\\n",
      "0                      0            False                 0      58:51.1   \n",
      "1    1560769551347900416            False                 0      58:51.1   \n",
      "2                      0            False                 0      58:51.1   \n",
      "3                      0            False                 0      58:51.1   \n",
      "4                      0            False                 0      58:51.1   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  breaking a michigan judge rules that prosecuto...  \n",
      "1  cbouzy filmystic yes we are not happy we are a...  \n",
      "2  the 19th amendment was ratified in 1920 granti...  \n",
      "3  aaronparnas patryanuc no to against every sing...  \n",
      "4  15 sandytxresister crystal4whales vtfishgirl1 ...  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell performs the following:\n",
    "Loads the tweet data from a CSV file into a pandas DataFrame.\n",
    "Defines a clean_text function to:\n",
    "    Convert text to lowercase.\n",
    "    Remove URLs, mentions, and hashtags.\n",
    "    Remove extra spaces and punctuations.\n",
    "Applies this function to clean the tweet texts.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('twitter_data.csv')\n",
    "\n",
    "# Function to clean the tweet text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # convert text to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # remove URLs\n",
    "    text = re.sub(r'\\@w+', '', text)  # remove mentions\n",
    "    text = re.sub(r'\\#\\w+', '', text)  # remove hashtags\n",
    "    text = re.sub(r'\\s+', ' ', text)  # replace multiple spaces with a single space\n",
    "    text = re.sub(r\"^\\s+|\\s+$\", \"\", text)  # remove spaces at the beginning and at the end of string\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuations\n",
    "    return text\n",
    "\n",
    "# Clean the tweets\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8091c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alancheung/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alancheung/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alancheung/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0         hashed_userid masked_username         location  \\\n",
      "0      415371  19868647935216335990       *****ecot              NaN   \n",
      "1      415370  95273352056344375133       *****kh59  Terre Haute, IN   \n",
      "2      415369  42256911176251501556  *******eDuster         Chi-town   \n",
      "3      415368  98949018742144878760       *****ll42              NaN   \n",
      "4      415367  83242079331442835051  *******tresist              NaN   \n",
      "\n",
      "   following  followers  totaltweets usercreateddt              tweetid  \\\n",
      "0       1109        796       189199       9/16/09  1560823411047268352   \n",
      "1        854        298        16999      10/10/17  1560823347583361024   \n",
      "2        416       8852        11699       3/26/09  1560823151671488513   \n",
      "3        603        179        59766       3/14/14  1560822898780213249   \n",
      "4        257        758       272531       2/16/17  1560822761706094592   \n",
      "\n",
      "  tweetcreatedts  ...  language favorite_count is_retweet  \\\n",
      "0        57:54.0  ...        en              0       True   \n",
      "1        57:39.0  ...        en              0      False   \n",
      "2        56:52.0  ...        en              0       True   \n",
      "3        55:52.0  ...        en              0       True   \n",
      "4        55:19.0  ...        en              0       True   \n",
      "\n",
      "     original_tweet_id  in_reply_to_status_id  is_quote_status  \\\n",
      "0  1560766115776565248                      0            False   \n",
      "1                    0    1560769551347900416            False   \n",
      "2  1560280488488718336                      0            False   \n",
      "3  1560750229720563712                      0            False   \n",
      "4  1560794896663203840                      0            False   \n",
      "\n",
      "   quoted_status_id  extractedts  \\\n",
      "0                 0      58:51.1   \n",
      "1                 0      58:51.1   \n",
      "2                 0      58:51.1   \n",
      "3                 0      58:51.1   \n",
      "4                 0      58:51.1   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  breaking a michigan judge rules that prosecuto...   \n",
      "1  cbouzy filmystic yes we are not happy we are a...   \n",
      "2  the 19th amendment was ratified in 1920 granti...   \n",
      "3  aaronparnas patryanuc no to against every sing...   \n",
      "4  15 sandytxresister crystal4whales vtfishgirl1 ...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  break michigan judg rule prosecutor enforc 193...  \n",
      "1                      cbouzi filmyst ye happi angri  \n",
      "2  19th amend ratifi 1920 grant women right vote ...  \n",
      "3        aaronparna patryanuc everi singl republican  \n",
      "4  15 sandytxresist crystal4whal vtfishgirl1 mary...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell will:\n",
    "Tokenize the tweets: Break down the text into individual words (tokens).\n",
    "Remove stopwords: Filter out common words like 'the', 'is', etc., that don't contribute much to sentiment.\n",
    "Apply stemming: Reduce words to their root form.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenization, Stopword Removal, and Stemming\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Optional: Stemming (you can also use Lemmatization based on your preference)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Apply the preprocessing to the cleaned text\n",
    "df['preprocessed_text'] = df['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9a7e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell processes emojis and slang.\n",
    "'''\n",
    "import emoji\n",
    "\n",
    "def convert_emojis_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\"\", \" \"))\n",
    "\n",
    "slang_dict = {\n",
    "    'lol': 'laughing out loud',\n",
    "    'brb': 'be right back',\n",
    "    'btw': 'by the way',\n",
    "    # Add more slang terms and their translations here\n",
    "}\n",
    "\n",
    "def translate_slang(text):\n",
    "    words = text.split()\n",
    "    translated_words = [slang_dict.get(word, word) for word in words]\n",
    "    return ' '.join(translated_words)\n",
    "\n",
    "\n",
    "# Apply emoji conversion\n",
    "df['text_with_emojis'] = df['preprocessed_text'].apply(convert_emojis_to_text)\n",
    "\n",
    "# Apply slang translation\n",
    "df['text_with_slang'] = df['text_with_emojis'].apply(translate_slang)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4899d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell will:\n",
    "Modify the preprocessed text to handle negations.\n",
    "Use the CountVectorizer from Scikit-Learn to convert the text data into a Bag of Words model,\n",
    "which represents text data in numerical form.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Function to handle negations\n",
    "def handle_negations(text):\n",
    "    negation_words = ['not', 'never', 'no']\n",
    "    words = text.split()\n",
    "    words_with_negation = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in negation_words and i+1 < len(words):\n",
    "            words_with_negation.append(words[i] + '_' + words[i+1])\n",
    "            i += 2  # Skip the next word\n",
    "        else:\n",
    "            words_with_negation.append(words[i])\n",
    "            i += 1\n",
    "    return ' '.join(words_with_negation)\n",
    "\n",
    "# Apply negation handling\n",
    "df['text_with_negations'] = df['text_with_slang'].apply(handle_negations)\n",
    "\n",
    "# Vectorization - Bag of Words\n",
    "vectorizer = CountVectorizer(max_features=1000)  # You can adjust the number of features\n",
    "X = vectorizer.fit_transform(df['text_with_negations'])\n",
    "\n",
    "# Now, X is a matrix representing the Bag of Words model of the text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a803a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 132)\t1\n",
      "  (0, 469)\t1\n",
      "  (0, 766)\t1\n",
      "  (0, 286)\t1\n",
      "  (0, 840)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 102)\t2\n",
      "  (0, 312)\t1\n",
      "  (0, 862)\t1\n",
      "  (0, 277)\t1\n",
      "  (0, 762)\t1\n",
      "  (0, 740)\t1\n",
      "  (0, 396)\t1\n",
      "  (0, 986)\t1\n",
      "  (0, 976)\t1\n",
      "  (1, 992)\t1\n",
      "  (1, 394)\t1\n",
      "  (1, 64)\t1\n",
      "  (2, 976)\t1\n",
      "  (2, 59)\t1\n",
      "  (2, 758)\t1\n",
      "  (2, 943)\t1\n",
      "  (2, 339)\t1\n",
      "  (2, 129)\t1\n",
      "  (3, 301)\t1\n",
      "  :\t:\n",
      "  (415370, 104)\t1\n",
      "  (415370, 253)\t1\n",
      "  (415370, 165)\t1\n",
      "  (415371, 758)\t1\n",
      "  (415371, 993)\t1\n",
      "  (415371, 974)\t1\n",
      "  (415371, 728)\t1\n",
      "  (415371, 164)\t1\n",
      "  (415371, 778)\t1\n",
      "  (415371, 532)\t1\n",
      "  (415371, 289)\t1\n",
      "  (415371, 928)\t1\n",
      "  (415371, 322)\t1\n",
      "  (415371, 686)\t1\n",
      "  (415371, 408)\t2\n",
      "  (415371, 642)\t1\n",
      "  (415371, 242)\t1\n",
      "  (415371, 427)\t1\n",
      "  (415371, 692)\t1\n",
      "  (415371, 17)\t1\n",
      "  (415371, 505)\t1\n",
      "  (415371, 897)\t2\n",
      "  (415371, 104)\t1\n",
      "  (415371, 253)\t1\n",
      "  (415371, 165)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28106e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
