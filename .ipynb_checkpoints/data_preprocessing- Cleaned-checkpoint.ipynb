{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e2269a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Twitter Data: Exploring Abortion Discourse\n",
    "by Alan Cheung and Bezawit Ayalew\n",
    "\n",
    "In this project, we perform sentiment analysis on a Twitter dataset focused on discussions surrounding abortion. Utilizing Python, we employ various natural language processing (NLP) techniques to analyze the sentiment expressed in tweets related to abortion. By processing and classifying the sentiment of these tweets, we aim to gain insights into public opinion and attitudes regarding this sensitive topic.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Dataset Loading and Cleaning\n",
    "\n",
    "This section focuses on loading and cleaning Twitter data from a CSV file. It performs operations to remove URLs, mentions, hashtags, extra spaces, leading/trailing spaces, and punctuations from tweet text. It also eliminates duplicate tweets based on text and user ID, and drops rows with empty tweet text.\n",
    "\n",
    "### Functions:\n",
    "\n",
    "The following functions are available:\n",
    "\n",
    "- `clean_text(text)`: Cleans the provided text.\n",
    "- `clean_tweets()`: Cleans the tweet text in the dataset, removes duplicates based on text and user ID, and drops rows with empty tweet text.\n",
    "- `display_cleaned_data()`: Displays the first few rows of the cleaned Twitter data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "576ae60c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TweetCleaner' object has no attribute 'original_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Displays and executes cleaning \u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     cleaner \u001b[38;5;241m=\u001b[39m TweetCleaner(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m     cleaner\u001b[38;5;241m.\u001b[39mclean_tweets()\n\u001b[1;32m     40\u001b[0m     cleaner\u001b[38;5;241m.\u001b[39mdisplay_cleaned_data()\n",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m, in \u001b[0;36mTweetCleaner.__init__\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TweetCleaner' object has no attribute 'original_df'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class TweetCleaner:\n",
    "    def __init__(self, file_path):\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.df = self.original_df.copy()\n",
    "        \n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()  # convert text to lowercase\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # remove URLs\n",
    "        text = re.sub(r'\\@w+', '', text)  # remove mentions\n",
    "        #text = re.sub(r'\\@\\w+', '', text)  # remove mentions\n",
    "        text = re.sub(r'\\#\\w+', '', text)  # remove hashtags\n",
    "        text = re.sub(r'\\s+', ' ', text)  # replace multiple spaces with a single space\n",
    "        text = re.sub(r\"^\\s+|\\s+$\", \"\", text)  # remove spaces at the beginning and at the end of string\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuations\n",
    "        return text\n",
    "\n",
    "    def clean_tweets(self):\n",
    "        self.df['cleaned_text'] = self.df['text'].apply(self.clean_text)\n",
    "        self.df.drop_duplicates(subset=['cleaned_text', 'hashed_userid'], inplace=True)\n",
    "        self.df.dropna(subset=['cleaned_text'], inplace=True)\n",
    "        self.df['tweetcreatedts'] = pd.to_datetime(self.df['tweetcreatedts'])  # Convert to datetime\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def display_cleaned_data(self):\n",
    "        print(self.df.head())\n",
    "        print(\"Total number of tweets cleaned:\", len(self.df))\n",
    "    \n",
    "    def load_original_data(self):\n",
    "        return self.original_df\n",
    "\n",
    "# Displays and executes cleaning \n",
    "if __name__ == \"__main__\":\n",
    "    cleaner = TweetCleaner('twitter_data.csv')\n",
    "    cleaner.clean_tweets()\n",
    "    cleaner.display_cleaned_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07cec9",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This Python code segment performs text preprocessing on tweets stored in a DataFrame. The preprocessing steps include tokenization, stopword removal, and stemming. The `preprocess_text()` function is defined to tokenize the text, remove stopwords, and stem the remaining words. The cleaned DataFrame is then used to apply the `preprocess_text()` function to the 'cleaned_text' column, and the preprocessed text is stored in a new column named 'preprocessed_text'. Finally, the first few rows of the DataFrame with preprocessed text are printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8091c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0         hashed_userid masked_username         location  \\\n",
      "0           0  13028308852725196659       *****roy1              NaN   \n",
      "1           1  45494576030177085457     ******eam_6              NaN   \n",
      "2           2  23641094384211702614     ******erlin  Los Angeles, CA   \n",
      "3           3  21587659761682762378    ******oshi23           Mumbai   \n",
      "4           4  47915146590553055505    ******4Truth              NaN   \n",
      "\n",
      "   following  followers  totaltweets usercreateddt              tweetid  \\\n",
      "0       9590       8725        29317    2012-01-24  1542763288815030273   \n",
      "1       8874      10090        33053    2019-02-07  1542763315926994945   \n",
      "2       7730      10006        35360    2008-08-07  1542763330670014464   \n",
      "3        232        132         4970    2016-05-10  1542763339738382337   \n",
      "4       6914       6310        49958    2021-04-02  1542763352279003136   \n",
      "\n",
      "       tweetcreatedts  ...  language favorite_count is_retweet  \\\n",
      "0 2022-07-01 06:53:25  ...        en              0       True   \n",
      "1 2022-07-01 06:53:32  ...        en              0       True   \n",
      "2 2022-07-01 06:53:35  ...        en              0       True   \n",
      "3 2022-07-01 06:53:38  ...        en              0       True   \n",
      "4 2022-07-01 06:53:40  ...        en              0       True   \n",
      "\n",
      "     original_tweet_id  in_reply_to_status_id  is_quote_status  \\\n",
      "0  1542689411489337344                      0            False   \n",
      "1  1542689411489337344                      0            False   \n",
      "2  1542689411489337344                      0            False   \n",
      "3  1542707138971242496                      0            False   \n",
      "4  1542689411489337344                      0            False   \n",
      "\n",
      "   quoted_status_id                 extractedts  \\\n",
      "0                 0  2022-07-01 12:03:31.185425   \n",
      "1                 0  2022-07-01 12:03:31.171429   \n",
      "2                 0  2022-07-01 12:03:31.157331   \n",
      "3                 0  2022-07-01 12:03:31.123276   \n",
      "4                 0  2022-07-01 12:03:31.097582   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  for 49 years libertys torch shone to ensure ba...   \n",
      "1  for 49 years libertys torch shone to ensure ba...   \n",
      "2  for 49 years libertys torch shone to ensure ba...   \n",
      "3  home based church are now everywhere in punjab...   \n",
      "4  for 49 years libertys torch shone to ensure ba...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  49 year liberti torch shone ensur basic human ...  \n",
      "1  49 year liberti torch shone ensur basic human ...  \n",
      "2  49 year liberti torch shone ensur basic human ...  \n",
      "3  home base church everywher punjab haryana even...  \n",
      "4  49 year liberti torch shone ensur basic human ...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Optional: Stemming (you can also use Lemmatization based on your preference)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cleaner = TweetCleaner('twitter_data.csv')\n",
    "    cleaned_df = cleaner.clean_tweets()\n",
    "\n",
    "    cleaned_df['preprocessed_text'] = cleaned_df['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "    print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b1c8a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis: Visualization and Insights\n",
    "\n",
    "This segment delves into exploratory data analysis (EDA) through visualization and insights:\n",
    "\n",
    "1. **Histogram of Text Lengths:** The function `plot_text_length_histogram()` generates a histogram displaying the distribution of tweet text lengths.\n",
    "\n",
    "2. **Bar Chart of Missing Values:** The function `plot_missing_values_bar_chart()` creates a bar chart illustrating the count of missing values in each column of the dataset.\n",
    "\n",
    "3. **Duplicate Tweets Handling:** The function `visualize_duplicate_tweets_before_after()` visualizes the number of tweets before and after the removal of duplicate entries. It also calculates and displays the total number of duplicate tweets removed.\n",
    "\n",
    "4. **Word Cloud Generation:** Two word clouds are generated: one before preprocessing using the original tweet text and another after preprocessing using the cleaned tweet text.\n",
    "\n",
    "5. **Saving Cleaned Data:** The DataFrame `df2` is created as a copy of the original DataFrame `df`, excluding the column 'cleaned_text'. This cleaned data is then saved to a CSV file named \"cleaned_data.csv\".\n",
    "\n",
    "6. **Histogram of Retweet Counts:** A histogram is plotted to visualize the distribution of retweet counts.\n",
    "\n",
    "7. **Time Series Analysis:** The tweet counts over time are visualized using a time series plot.\n",
    "\n",
    "8. **Top Languages Bar Plot:** A bar plot displays the top 5 languages used in the tweets.\n",
    "\n",
    "9. **Histogram of Followers:** A histogram illustrates the distribution of the number of followers.\n",
    "\n",
    "10. **Summary Statistics:** Summary statistics are calculated and displayed for the number of followers, retweet count, and tweet length.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9a7e105",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TweetCleaner' object has no attribute 'original_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     cleaner \u001b[38;5;241m=\u001b[39m TweetCleaner(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     96\u001b[0m     original_df \u001b[38;5;241m=\u001b[39m cleaner\u001b[38;5;241m.\u001b[39moriginal_df  \u001b[38;5;66;03m# Accessing the original_df attribute of TweetCleaner\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     cleaned_df \u001b[38;5;241m=\u001b[39m cleaner\u001b[38;5;241m.\u001b[39mclean_tweets()\n",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m, in \u001b[0;36mTweetCleaner.__init__\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TweetCleaner' object has no attribute 'original_df'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class TweetVisualizer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def plot_text_length_histogram(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(self.df['cleaned_text'].str.len(), bins=30, color='skyblue', edgecolor='black')\n",
    "        plt.title('Histogram of Tweet Text Lengths')\n",
    "        plt.xlabel('Text Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_missing_values_bar_chart(self):\n",
    "        missing_values_count = self.df.isnull().sum()\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        missing_values_count.plot(kind='bar', color='salmon')\n",
    "        plt.title('Bar Chart of Missing Values')\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Missing Values Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_duplicate_tweets_before_after(self, original_df):\n",
    "        total_tweets_before = len(original_df)\n",
    "        df_cleaned = self.df.drop_duplicates(subset=['cleaned_text', 'hashed_userid'])\n",
    "        total_tweets_after = len(df_cleaned)\n",
    "        total_duplicates_removed = total_tweets_before - total_tweets_after\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(['Before Removal', 'After Removal'], [total_tweets_before, total_tweets_after], color=['skyblue', 'salmon'], edgecolor='black')\n",
    "        plt.title('Number of Tweets Before and After Duplicate Removal')\n",
    "        plt.ylabel('Count')\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "        print(f\"Total duplicate tweets removed: {total_duplicates_removed}\")\n",
    "\n",
    "    \n",
    "    def generate_word_cloud(self, text, title, color):\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', colormap=color).generate(text)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_retweet_counts_histogram(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(self.df['followers'], bins=100, kde=False)\n",
    "        plt.title('Distribution of Retweet Counts')\n",
    "        plt.xlabel('Retweet Count')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_tweet_counts_over_time(self):\n",
    "        self.df['tweetcreatedts'] = pd.to_datetime(self.df['tweetcreatedts'])\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        self.df.resample('D', on='tweetcreatedts').count()['tweetid'].plot()\n",
    "        plt.title('Tweet Counts Over Time')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Number of Tweets')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def plot_top_languages_bar_chart(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(y='language', data=self.df, order=self.df['language'].value_counts().iloc[:5].index)\n",
    "        plt.title('Top 5 Languages Used in Tweets')\n",
    "        plt.xlabel('Number of Tweets')\n",
    "        plt.ylabel('Language')\n",
    "        plt.show()\n",
    "   \n",
    "    def plot_followers_distribution(self):\n",
    "        pd.set_option('display.float_format', '{:.1f}'.format)\n",
    "\n",
    "        # Summary statistics for the number of followers\n",
    "        followers_summary = self.df['followers'].describe().round(1)\n",
    "        print(\"Summary Statistics for Followers:\\n\", followers_summary)\n",
    "        print(\"Total number of entries:\", len(self.df['followers']))\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(self.df['followers'], bins=30, kde=False, color='skyblue')\n",
    "        plt.title('Distribution of Followers')\n",
    "        plt.xlabel('Number of Followers')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xscale('log')\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cleaner = TweetCleaner('twitter_data.csv')\n",
    "    original_df = cleaner.original_df\n",
    "    cleaned_df = cleaner.clean_tweets()\n",
    "    \n",
    "\n",
    "    visualizer = TweetVisualizer(cleaned_df)\n",
    "    #visualizer.plot_text_length_histogram()\n",
    "    #visualizer.plot_missing_values_bar_chart()\n",
    "   \n",
    "    visualizer.visualize_duplicate_tweets_before_after(orginal_df)\n",
    "    #tweet_text_before_preprocessing = ' '.join(cleaned_df['text'].dropna())\n",
    "    #visualizer.generate_word_cloud(tweet_text_before_preprocessing, 'Word Cloud Before Preprocessing', 'viridis')\n",
    "\n",
    "    #tweet_text_after_preprocessing = ' '.join(cleaned_df['cleaned_text'].dropna())\n",
    "    #visualizer.generate_word_cloud(tweet_text_after_preprocessing, 'Word Cloud After Preprocessing', 'plasma')\n",
    "\n",
    "    visualizer.plot_retweet_counts_histogram()\n",
    "    #visualizer.plot_tweet_counts_over_time()\n",
    "    #isualizer.plot_top_languages_bar_chart()\n",
    "    visualizer.plot_followers_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c210f",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Engineering: Text Preprocessing and Vectorization\n",
    "\n",
    "This cell implements various feature engineering techniques to prepare the text data for sentiment analysis and other tasks:\n",
    "\n",
    "1. **Negation Handling:** The `handle_negations()` function modifies the preprocessed text to handle negations by appending a special tag to negate the following word.\n",
    "   \n",
    "2. **Sentiment Analysis Preparation:** Custom lists of positive and negative words are defined for sentiment analysis.\n",
    "\n",
    "3. **Sentiment Analysis:** The `assign_sentiment()` function assigns sentiment scores to each tweet based on the presence of positive and negative words.\n",
    "\n",
    "4. **Vectorization - Bag of Words (BoW):** The `CountVectorizer` from Scikit-Learn converts the text data into a Bag of Words model, which represents text data in numerical form.\n",
    "\n",
    "5. **Vectorization - TF-IDF:** The `TfidfVectorizer` from Scikit-Learn converts the text data into TF-IDF (Term Frequency-Inverse Document Frequency) representation, which is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents.\n",
    "\n",
    "6. **Word Embeddings with Word2Vec:** The tokenized text is used to train a Word2Vec model, and then each tweet is represented as an average of word vectors. These embeddings capture semantic relationships between words in the text data.\n",
    "\n",
    "7. **PCA Transformation (Optional):** The PCA (Principal Component Analysis) transformation can be applied to reduce the dimensionality of the word embeddings if needed.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4899d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Function to handle negations\n",
    "def handle_negations(text):\n",
    "    negation_words = ['not', 'never', 'no']\n",
    "    words = text.split()\n",
    "    words_with_negation = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in negation_words and i+1 < len(words):\n",
    "            words_with_negation.append(words[i] + '_' + words[i+1])\n",
    "            i += 2  # Skip the next word\n",
    "        else:\n",
    "            words_with_negation.append(words[i])\n",
    "            i += 1\n",
    "    return ' '.join(words_with_negation)\n",
    "\n",
    "# Apply negation handling\n",
    "df['text_with_negations'] = df['text_with_slang'].apply(handle_negations)\n",
    "\n",
    "\n",
    "# SENTIMENT PROCESSING SECTION \n",
    "\n",
    "# Custom lists of positive and negative words\n",
    "positive_words = ['good', 'great', 'excellent', 'awesome', 'fantastic', 'amazing', 'superb', 'wonderful', 'outstanding', 'terrific', 'pleasure', 'joy', 'love', 'happy', 'delight', 'satisfied', 'excited', 'admire', 'brilliant', 'kind', 'grateful', 'uplifting']\n",
    "negative_words = ['bad', 'terrible', 'horrible', 'awful', 'disappointing', 'unpleasant', 'poor', 'inferior', 'disgusting', 'ugly', 'disgraceful', 'dreadful', 'sad', 'depressed', 'angry', 'hate', 'annoying', 'frustrating', 'unacceptable', 'regret', 'boring', 'annoyed', 'unhappy', 'displeased', 'upset']\n",
    "\n",
    "\n",
    "# Function to assign sentiment scores\n",
    "def assign_sentiment(text):\n",
    "    sentiment_score = 0\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            sentiment_score += 1\n",
    "        elif word in negative_words:\n",
    "            sentiment_score -= 1\n",
    "    return sentiment_score\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df['sentiment_score'] = df['text_with_negations'].apply(assign_sentiment)\n",
    "\n",
    "#End \n",
    "\n",
    "# Vectorization - Bag of Words\n",
    "vectorizer = CountVectorizer(max_features=1000)  # You can adjust the number of features\n",
    "X = vectorizer.fit_transform(df['text_with_negations'])\n",
    "\n",
    "# Now, X is a matrix representing the Bag of Words model of the text data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorization - TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust the number of features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['text_with_negations'])\n",
    "\n",
    "# Now, X_tfidf is a matrix representing the TF-IDF model of the text data\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize text into list of words\n",
    "tokenized_text = df['text_with_negations'].apply(lambda x: x.split())\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word_vectors = word2vec_model.wv\n",
    "\n",
    "# Function to average word vectors for a sentence\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "# Apply averaging to each sentence\n",
    "word_embeddings = []\n",
    "for text in tokenized_text:\n",
    "    word_embeddings.append(average_word_vectors(text, word2vec_model, word_vectors, 100))\n",
    "\n",
    "X_word_embedding = np.array(word_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
